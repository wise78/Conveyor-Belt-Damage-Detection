# -*- coding: utf-8 -*-
"""ConvertEdgetpu

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jKUWHh1c3ddMw3sWIif8e_S6ZOtw39zv
"""

# ===========================
# ‚úÖ ALL-IN-ONE COLAB CELL
# ===========================
# - Upload ZIP dataset (berisi train/valid/test + images/labels)
# - Upload model YOLO11n hasil training (.pt)
# - Auto-detect struktur dataset, generate data.yaml
# - Export ke EdgeTPU (INT8) dengan imgsz=320
# - Verifikasi input tensor shape dan siapkan file untuk diunduh
# ===========================

# --- 0) Setup & install deps (Ultralytics + TF + ONNX toolchain) ---
!pip -q install "ultralytics==8.3.185" "tensorflow==2.19.0" onnx onnxslim

import os, re, zipfile, glob, shutil, json
from pathlib import Path
from collections import Counter, defaultdict
from google.colab import files

print("‚úÖ Deps installed. Ultralytics, TF, ONNX ready.")

# --- 1) Upload files (dataset ZIP + model .pt) ---
print("\nüì§ Silakan upload ZIP dataset kamu...")
uploaded_ds = files.upload()  # pilih ZIP dataset
assert len(uploaded_ds) >= 1, "ZIP dataset wajib diupload!"
DATASET_ZIP = list(uploaded_ds.keys())[0]
assert DATASET_ZIP.lower().endswith((".zip",)), "Harus ZIP untuk dataset!"

print("\nüì§ Sekarang upload model YOLO (.pt) hasil training...")
uploaded_model = files.upload()
assert len(uploaded_model) >= 1, "Model .pt wajib diupload!"
MODEL_PT = list(uploaded_model.keys())[0]
assert MODEL_PT.lower().endswith(".pt"), "Harus file .pt untuk model!"

# --- 2) Paths & constants ---
ROOT = Path("/content")
DATASET_DIR = ROOT / "datasets" / "conveyor"
DATASET_DIR.mkdir(parents=True, exist_ok=True)
MODEL_PT_PATH = ROOT / MODEL_PT
IMG_EXTS = {".jpg", ".jpeg", ".png", ".bmp", ".webp", ".tiff", ".tif"}
LABEL_EXT = ".txt"

print(f"\nüì¶ Dataset ZIP: {DATASET_ZIP}")
print(f"üß† Model PT   : {MODEL_PT_PATH}")

# --- 3) Unzip dataset ---
print("\nüîì Unzipping dataset...")
with zipfile.ZipFile(ROOT / DATASET_ZIP, 'r') as zf:
    zf.extractall(DATASET_DIR)
print("‚úÖ Unzip selesai:", DATASET_DIR)

# --- 4) Auto-detect struktur train/val/test + images/labels ---
def which_split(p: Path):
    s = "/".join([x.lower() for x in p.parts])
    if re.search(r'/(train|training)(/|$)', s): return "train"
    if re.search(r'/(valid|val|validation)(/|$)', s): return "val"
    if re.search(r'/(test|testing)(/|$)', s): return "test"
    return None

candidates = []
for root, dirs, files_list in os.walk(DATASET_DIR):
    imgs = [f for f in files_list if Path(f).suffix.lower() in IMG_EXTS]
    lbls = [f for f in files_list if Path(f).suffix.lower() == LABEL_EXT]
    if imgs or lbls:
        candidates.append({
            "path": str(Path(root)),
            "contains_images": bool(imgs),
            "contains_labels": bool(lbls),
            "n_images": len(imgs),
            "n_labels": len(lbls),
        })

by_split = defaultdict(lambda: {"images": [], "labels": []})
for c in candidates:
    p = Path(c["path"])
    split = which_split(p)
    if not split:
        continue
    if c["contains_images"]:
        by_split[split]["images"].append(c)
    if c["contains_labels"]:
        by_split[split]["labels"].append(c)

def pick_best(entries):
    if not entries: return None
    return max(entries, key=lambda e: (e["n_images"] if e["contains_images"] else e["n_labels"], len(e["path"])))

picked = {}
for split in ("train", "val", "test"):
    imgs_entry = pick_best(by_split[split]["images"])
    lbls_entry = pick_best(by_split[split]["labels"])
    picked[split] = {
        "images": imgs_entry["path"] if imgs_entry else None,
        "labels": lbls_entry["path"] if lbls_entry else None
    }

def try_pair_from_images(img_dir):
    if not img_dir: return None
    p = Path(img_dir)
    if p.name.lower() == "images" and p.parent.exists():
        sib = p.parent / "labels"
        if sib.exists():
            return str(sib)
    s2 = re.sub(r'images', 'labels', str(p), flags=re.IGNORECASE)
    if s2 != str(p) and Path(s2).exists():
        return s2
    return None

for sp in ("train","val","test"):
    if picked[sp]["images"] and not picked[sp]["labels"]:
        g = try_pair_from_images(picked[sp]["images"])
        if g: picked[sp]["labels"] = g

# --- 5) Infer nc & names (dari label atau data.yaml lama bila ada) ---
def infer_nc_and_counts(label_dir):
    max_cls = -1
    counts = Counter()
    if not label_dir:
        return None, counts
    for root, dirs, files_list in os.walk(label_dir):
        for f in files_list:
            if Path(f).suffix.lower() != ".txt": continue
            with open(Path(root)/f, "r") as fh:
                for line in fh:
                    line=line.strip()
                    if not line: continue
                    parts=line.split()
                    try:
                        cid = int(float(parts[0]))
                        max_cls = max(max_cls, cid)
                        counts[cid]+=1
                    except:
                        pass
    return (max_cls+1 if max_cls>=0 else None), counts

nc, counts = None, Counter()
for sp in ("train","val","test"):
    nci, cnt = infer_nc_and_counts(picked[sp]["labels"])
    if nci is not None:
        nc = nci
        counts.update(cnt)
        break

# Coba baca names dari data.yaml yang mungkin ada di zip
names = None
found_yaml = []
for root, dirs, files_list in os.walk(DATASET_DIR):
    for f in files_list:
        if f.lower() in ("data.yaml", "dataset.yaml"):
            found_yaml.append(str(Path(root)/f))

def parse_names_from_yaml(p):
    try:
        import yaml
    except ImportError:
        !pip -q install pyyaml
        import yaml
    try:
        with open(p,"r") as fh:
            y = yaml.safe_load(fh)
        if isinstance(y, dict) and "names" in y and isinstance(y["names"], list) and y["names"]:
            return y["names"]
    except:
        return None

for yp in found_yaml:
    nm = parse_names_from_yaml(yp)
    if nm:
        names = nm
        break

if names is None and nc is not None:
    names = [f"class{i}" for i in range(nc)]

if nc is None and names is not None:
    nc = len(names)
if nc is None:
    nc, names = 1, ["class0"]

# --- 6) Tentukan root 'path' & rel paths untuk YAML ---
def common_root(paths):
    parts = [Path(p).parts for p in paths if p]
    if not parts: return str(DATASET_DIR)
    min_len = min(len(p) for p in parts)
    prefix=[]
    for i in range(min_len):
        toks={p[i] for p in parts}
        if len(toks)==1: prefix.append(parts[0][i])
        else: break
    return str(Path(*prefix)) if prefix else str(DATASET_DIR)

img_dirs = [picked[s]["images"] for s in ("train","val","test") if picked[s]["images"]]
lbl_dirs = [picked[s]["labels"] for s in ("train","val","test") if picked[s]["labels"]]
root_guess = common_root(img_dirs + lbl_dirs)

def rel_or_abs(root, p):
    if not p: return None
    try:
        return str(Path(p).resolve().relative_to(Path(root).resolve()))
    except:
        return str(Path(p))

yaml_obj = {"path": root_guess, "nc": int(nc), "names": names}
for sp in ("train","val","test"):
    img_dir = picked[sp]["images"]
    if img_dir:
        yaml_obj[sp] = rel_or_abs(root_guess, img_dir)

DATA_YAML_PATH = ROOT / "data.yaml"
try:
    import yaml
except ImportError:
    !pip -q install pyyaml
    import yaml

with open(DATA_YAML_PATH, "w") as f:
    yaml.safe_dump(yaml_obj, f, sort_keys=False, allow_unicode=True)

print("\nüßæ Generated data.yaml:")
print(DATA_YAML_PATH.read_text())

# --- 7) Export ke EdgeTPU (INT8) dengan imgsz=320 ---
from ultralytics import YOLO
print("\nüöÄ Export EdgeTPU (INT8) dengan imgsz=256...")
model = YOLO(str(MODEL_PT_PATH))
export_res = model.export(format="edgetpu", imgsz=256, data=str(DATA_YAML_PATH))
print("‚úÖ Export selesai.")

# Cari file output *_edgetpu.tflite
tflite_files = sorted(glob.glob(str(ROOT / "*edgetpu.tflite"))) + \
               sorted(glob.glob(str(Path.cwd() / "*edgetpu.tflite")))
if not tflite_files:
    # Ultralytics kadang menyimpan di cwd
    tflite_files = glob.glob("*edgetpu.tflite")
assert tflite_files, "Tidak menemukan file *_edgetpu.tflite (cek log export)."
TFLITE_PATH = tflite_files[0]
print("üìÑ TFLite (EdgeTPU):", TFLITE_PATH)

# --- 8) Verifikasi input tensor shape (should be [1, 320, 320, 3]) ---
from tensorflow.lite import Interpreter as TfLiteInterpreter
interp = TfLiteInterpreter(model_path=TFLITE_PATH)
interp.allocate_tensors()
inp = interp.get_input_details()[0]
print("üîé Input tensor shape:", inp["shape"])

# --- 9) Siapkan untuk diunduh ke lokal (PC mu) ---
print("\n‚¨áÔ∏è  Silakan unduh file model di bawah ini (klik output link):")
files.download(TFLITE_PATH)

# Ringkasan
summary = {
    "data_yaml": str(DATA_YAML_PATH),
    "dataset_root": root_guess,
    "splits": {k: {"images": picked[k]["images"], "labels": picked[k]["labels"]} for k in ("train","val","test")},
    "nc": nc,
    "names": names,
    "tflite_edgetpu": TFLITE_PATH,
    "input_shape": inp["shape"].tolist()
}
print("\n=== SUMMARY ===")
print(json.dumps(summary, indent=2, ensure_ascii=False))